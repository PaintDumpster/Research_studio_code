{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[0;32m      4\u001b[0m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m      5\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    " # type: ignore\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "load_dotenv()\n",
    "\n",
    "# %%\n",
    "# Instead of using Azure OpenAI, we'll use a locally hosted Llama model\n",
    "# For this example, we'll assume you have the Llama 3 (8B) model and tokenizer available.\n",
    "# Note: Replace \"path/to/llama3-8b-8192\" with the actual path or HuggingFace Hub model name.\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n",
    "\n",
    "model_name = \"path/to/llama3-8b-8192\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "model = LlamaForCausalLM.from_pretrained(model_name, device_map=\"auto\", low_cpu_mem_usage=True)\n",
    "\n",
    "# Set generation parameters\n",
    "gen_config = GenerationConfig(\n",
    "    temperature=0.0,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "# %%\n",
    "# In the original code, AzureChatOpenAI was used with SystemMessage and HumanMessage.\n",
    "# Here, we'll simulate that with a simple prompt format. \n",
    "# We'll combine system and human messages into one prompt.\n",
    "\n",
    "system_message = \"You are a comedian\\n\"\n",
    "human_message = \"Hello\"\n",
    "\n",
    "prompt = f\"{system_message}User: {human_message}\\nAssistant:\"\n",
    "\n",
    "# Tokenize the prompt\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate the response\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=gen_config.max_new_tokens,\n",
    "        temperature=gen_config.temperature,\n",
    "        do_sample=gen_config.do_sample\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(output_ids[0][len(input_ids[0]):], skip_special_tokens=True)\n",
    "print(response)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Test the embedding model\n",
    "#\n",
    "# The original code tested embeddings using the AzureOpenAI embeddings API.\n",
    "# For a Llama model, embeddings are not typically provided directly as text embeddings.\n",
    "# Instead, you can use a separate embedding model, such as one from the sentence-transformers library.\n",
    "\n",
    "# %%\n",
    "# Example: Using a sentence-transformers model for embeddings.\n",
    "# Note: Install sentence-transformers: `pip install sentence-transformers`\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedder = SentenceTransformer(embedding_model_name)\n",
    "\n",
    "text = \"Your text string goes here\"\n",
    "embedding = embedder.encode(text)\n",
    "print(embedding)\n",
    "\n",
    "# %%\n",
    "# The embedding is now available as a numpy array.\n",
    "# This can be used similarly to how Azure embeddings were used in the original code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
